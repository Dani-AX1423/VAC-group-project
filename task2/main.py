# -*- coding: utf-8 -*-
"""task2.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pWStoa5P-ZrPM51gJpwuJVK7tzsK_B2U
"""

# Import spacy, PyPDF2 and random modules
import spacy
import PyPDF2
import random
from spacy import displacy
from spacy.matcher import Matcher

# Load the English model
nlp = spacy.load("en_core_web_sm")

# Select a random sentence from the pdf
pdf_file = "/content/US_Declaration.pdf"
with open(pdf_file, "rb") as f:
    reader = PyPDF2.PdfReader(f)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + " "

# Process the PDF text
doc1 = nlp(text)

# To select a random sentence
sentences = list(doc1.sents)
ran_sen = str(random.choice(sentences))
print("\nRandom Sentence:\n" + ran_sen)

# Process the sentence
doc = nlp(ran_sen)

# Print entities
print("\nEntities found in the sentence:\n")
for ent in doc.ents:
    print(f"Text: {ent.text}, Label: {ent.label_}")

# To identify noun chunks from a sentence
print("\nNoun Chunks in the sentence:\n")
for chunk in doc.noun_chunks:
    print(chunk.text)

print("\nThe displacy diagram:\n")
# Display the dependency parse (noun chunks are highlighted as part of this)
displacy.render(doc, style="dep", jupyter=True, options={"compact": True, "bg": "white", "color": "blue", "font": "Source Sans Pro"})

# Initialize the Matcher
matcher = Matcher(nlp.vocab)

# Define a simple pattern:
pattern = [
    {"POS": "PROPN"},      # July
    {"LIKE_NUM": True},    # 4
    {"IS_PUNCT": True, "OP": "?"},  # ,
    {"LIKE_NUM": True}     # 1776
]
matcher.add("DATE", [pattern])

# Find matches
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end]
    print(f"Matched pattern: {span.text}")

